import os
import glob
import re
import copy
import shutil
import json
import boto3
from botocore.exceptions import NoCredentialsError
from concurrent.futures import ThreadPoolExecutor
import datetime
import shelve
import streamlit as st
import numpy as np
import pandas as pd
from pathlib import Path
from PIL import Image
# from src.logger import my_logger
import random


@st.cache
def list_imagespath(image_dir):
    images_fullpath = glob.glob(os.path.join(image_dir, "*"))
    images_fullpath = [folder for folder in images_fullpath if os.path.isdir(folder)]
    images_path = [os.path.basename(folder) for folder in images_fullpath]
    images_path.sort()
    return images_path


def list_imagespath_nonCache(image_dir):
    """ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ç”¨ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹ç‰ˆï¼‰
    """
    images_fullpath = glob.glob(os.path.join(image_dir, "*"))
    images_fullpath = [folder for folder in images_fullpath if os.path.isdir(folder)]
    images_path = [os.path.basename(folder) for folder in images_fullpath]
    images_path.sort()
    return images_path


# @st.cache
def list_images(target_dir):
    base_images = glob.glob(target_dir + "/*.jpg")
    base_images.sort()
    return base_images


def list_csvs(target_dir):                                       # 2024.5.21 -->
    base_csvs = glob.glob(target_dir + "/*.csv")
    base_csvs.sort()
    return base_csvs                                             # --> 2024.5.21


@st.cache
def get_dir_list(path):
    dir_path = Path(path)
    dir_obj_list = [path for path in dir_path.glob("*") if path.is_dir() and not path.name.startswith(".")]
    dir_list = [image_obj.name for image_obj in dir_obj_list]
    dir_list.sort()
    return dir_list


@st.cache
def get_file_list(path):
    dir_path = Path(path)
    image_obj_list = [path for path in dir_path.glob("*") if path.is_file()]
    image_list = [image_obj.name for image_obj in image_obj_list]
    image_list.sort()
    return image_list


@st.cache
def read_markdown_file(markdown_file):
    return Path(markdown_file).read_text()


@st.cache
def read_python_file(python_file):
    return Path(python_file).read_text()


@st.cache
def get_file_content_as_string(filename):
    st.code(filename)


@st.cache
def ohc_image_load(base_images, idx, caption):
    st.text(f'{idx}ç•ªç›®ã®ç”»åƒã‚’è¡¨ç¤ºã—ã¾ã™')
    im_base = Image.open(base_images[idx])
    st.image(im_base, caption=caption)
    return im_base


@st.cache
def rail_name_to_jp(rail_name, config):
    return config.rail_names[rail_name]


@st.cache
def station_name_to_jp(st_name, config):
    return config.station_names[st_name]


@st.cache
def rail_type_to_jp(rail_type, config):
    return config.rail_type_names[rail_type]


@st.cache
def rail_type_name_to_jp(time_band_names, config):
    return config.time_band_names[time_band_names]


@st.cache
def camera_num_to_name(camera_num, config):
    return config.camera_names[camera_num]


# @st.cache
def rail_message(dir_area, config):
    str_list = re.split('[_-]', dir_area)
    rail_name = rail_name_to_jp(str_list[0], config)
    if str_list[3] == 'St' or str_list[3] == 'st':
        st_name = station_name_to_jp(str_list[2], config) + 'æ§‹å†…'
    else:
        st_name = station_name_to_jp(str_list[2], config) + 'ï½' + station_name_to_jp(str_list[3], config)
    updown_name = rail_type_to_jp(str_list[4], config)
    date_obj = datetime.datetime.strptime(str_list[5], "%Y%m%d")
    measurement_date = date_obj.strftime("%Yå¹´%mæœˆ%dæ—¥")    # # yyyyå¹´mmæœˆddæ—¥å½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
    measurement_time = rail_type_name_to_jp(str_list[6], config)
    return rail_name, st_name, updown_name, measurement_date, measurement_time


@st.cache
def rail_camera_initialize(rail, camera_num, base_images, trolley_ids):
    """ railã«æ›¸ãè¾¼ã‚ã‚‹ã‚ˆã†ã«åˆæœŸåŒ–ã™ã‚‹
        è§£æçµæœãŒæ—¢ã«ã‚ã‚‹å ´åˆã¯åˆæœŸåŒ–ã—ãªã„
    Args:
        rail (shelve): è§£æçµæœä¿å­˜ç”¨ã®shelveãƒ•ã‚¡ã‚¤ãƒ«
        camera_num (str): ã‚«ãƒ¡ãƒ©ç•ªå·
        base_images (str): ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        trolley_ids (str): trolley_idã®ãƒ†ãƒ³ãƒ—ãƒ¬ (trolley1, trolley2 ...)
    """
    # if len(rail) < 2 or not any(len(rail[camera_num].get(image_path, {})) > 0 for image_path in base_images):
    #     print('rail initilize')
    #     print(f'dir_area: {rail["name"]}')
    #     # railã‚’åˆæœŸåŒ–
    #     rail[camera_num] = {image_path: {trolley_id: {} for trolley_id in trolley_ids} for image_path in base_images}
    
    # ä¿®æ­£å‰ã®ã‚³ãƒ¼ãƒ‰ğŸ‘‡
    if len(rail) < 2:    # åˆã‚ã¦railãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆã¯"name"ã ã‘ãªã®ã§len(rail)ã¯1
        rail_check = False
    else:    # ä¸€åº¦ã§ã‚‚è§£æã•ã‚Œã‚‹ã¨trolley_idãŒè¿½åŠ ã•ã‚Œã‚‹ãŸã‚1ä»¥ä¸Š
        # rail_check = any(len(rail[camera_num][image_path]) > 0 for image_path in base_images)
        rail_check = any(key in image_path for key in rail[camera_num].keys() for image_path in base_images)
    if not rail_check:
        # print('rail initilize')
        # print(f'dir_area: {rail["name"]}')
        # railã‚’åˆæœŸåŒ–
        # base_imagesã¨åŒã˜é•·ã•ã®ç©ºã®dictionaryã‚’ä½œæˆã—ã¦railã‚’åˆæœŸåŒ–
        blankdict_size = [{}] * len(base_images)
        rail[camera_num] = dict(zip(base_images, blankdict_size))
        # trolley_idsã¨åŒã˜é•·ã•ã®ç©ºã®dictionaryã‚’ä½œæˆã—ã¦railã‚’åˆæœŸåŒ–
        blankdict_size = [{}] * len(trolley_ids)
        for image_path in base_images:
            rail[camera_num][image_path] = dict(zip(trolley_ids, blankdict_size))
    return


# @st.experimental_singleton(show_spinner=True)
def get_s3_dir_list(path):
    """ S3ãƒã‚±ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’å–å¾—ã™ã‚‹
    Args:
        path (str): backetå†…ã®ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå(ä¾‹)imgs
    """
    backet_name = "trolley-monitor"

    s3 = boto3.client('s3')
    rail_list = []
    # S3ãƒã‚±ãƒƒãƒˆå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’å–å¾—
    response = s3.list_objects_v2(Bucket=backet_name, Prefix=path + "/" , Delimiter='/')
    for content in response.get('CommonPrefixes', []):
        full_path = content.get('Prefix')
        normalized_path = os.path.normpath(full_path)
        rail_list.append(os.path.basename(normalized_path))
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒ1000ä»¶æœªæº€ã«ãªã‚‹ã¾ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç¶šã‘ã‚‹
    while response['IsTruncated']:
        response = s3.list_objects_v2(Bucket=backet_name, Prefix=path + "/" , Delimiter='/', ContinuationToken=response['NextContinuationToken'])
        for content in response.get('CommonPrefixes', []):
            full_path = content.get('Prefix')
            normalized_path = os.path.normpath(full_path)
            rail_list.append(os.path.basename(normalized_path))
    rail_list.sort()
    return rail_list


def get_s3_image_list(path):
    """ S3ãƒã‚±ãƒƒãƒˆã®ã‚«ãƒ¡ãƒ©ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹
    Args:
        path (str): backetã‚’èµ·ç‚¹ã¨ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹
                    (ä¾‹)images/Chuo_01_Tokyo-St_20230201_knight/HD11/
    """
    backet_name = "trolley-monitor"

    s3 = boto3.client('s3')
    image_list = []

    # S3ãƒã‚±ãƒƒãƒˆå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    response = s3.list_objects_v2(Bucket=backet_name, Prefix=path + "/")
    for content in response.get('Contents', []):
        full_path = content.get('Key')
        image_list.append(os.path.basename(full_path))
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒ1000ä»¶æœªæº€ã«ãªã‚‹ã¾ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç¶šã‘ã‚‹
    while response['IsTruncated']:
        response = s3.list_objects_v2(Bucket=backet_name, Prefix=path + "/", ContinuationToken=response['NextContinuationToken'])
        for content in response.get('Contents', []):
            full_path = content.get('Key')
            image_list.append(os.path.basename(full_path))
    image_list.sort()
    return image_list


def list_csv_files(bucket_name, prefix):
    """ S3ã«ã‚ã‚‹CSVã®ä¸€è¦§ã‚’ã‚²ãƒƒãƒˆ
    """
    s3 = boto3.client('s3')
    paginator = s3.get_paginator('list_objects_v2')

    csv_files = []
    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.csv'):
                    csv_files.append(obj['Key'])

    return csv_files


# @st.experimental_singleton(show_spinner=True)
def get_S3_file_content_as_string(img_dir_name, path):
    """ S3ãƒã‚±ãƒƒãƒˆã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¦ãã‚‹
    Args:
        img_dir_name (str): backetå†…ã®ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå(ä¾‹)imgs/
        path (str): ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå(ä¾‹)Chuo_01_Tokyo-St_20230201_knight/
    """
    backet_name = "trolley-monitor"
    dir_path = img_dir_name + path

    s3 = boto3.resource('s3')
    response = s3.Object(backet_name, dir_path).get()['Body']
    return response.read().decode("utf-8")


def get_columns_from_csv(bucket_name, columns_csv_key):
    """ S3ã‹ã‚‰ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆã‚’ã‚²ãƒƒãƒˆ
    """
    s3 = boto3.client('s3')
    obj = s3.get_object(Bucket=bucket_name, Key=columns_csv_key)
    columns_df = pd.read_csv(obj['Body'])
    return columns_df.columns.tolist()


def get_KiroTei_dict(config, df):
    """ èµ°è¡Œæ—¥ãƒ»ç·šåŒºã”ã¨ã®ã‚­ãƒ­ç¨‹è£œæ­£æƒ…å ±ã‚’è¾æ›¸ã«è¨˜éŒ²ã™ã‚‹
    """
    # ã‚«ãƒ¡ãƒ©ç•ªå·ã”ã¨ã«ã€EkiCdã®ã¯ã˜ã‚ã®è¡Œã‚’Trueã¨ã—ã¦è¨˜éŒ²ã™ã‚‹
    for camera_num in config.camera_types:
        df[f"EkiCdDiff{camera_num}"] = df.query(f"GazoFileName{camera_num}.notnull()", engine='python')['EkiCd'].diff().map(lambda x: x != 0)
    # èµ°è¡Œæ—¥ãƒ»ç·šåŒºã”ã¨ã«ã€ç”»åƒ1æšã‚ãŸã‚Šã®å¹…(m)ã‚’ç®—å‡ºã—ã¦è¾æ›¸ã«è¨˜éŒ²ã™ã‚‹
    result_dict = {}
    for camera_num in config.camera_types:
        grouped_df = df.query(f"GazoFileName{camera_num}.notnull()", engine='python').groupby(['SokuteiDate', 'EkiCd'])['KiroTei'].count()
        for (date, ekicd), count in grouped_df.items():
            if date not in result_dict:
                result_dict[date] = {}
            if ekicd not in result_dict[date]:
                result_dict[date][ekicd] = {}
            # print(f"camera_num: {camera_num}, date: {date}, ekicd: {ekicd}")
            KiroTei_head = df.query(f"EkiCdDiff{camera_num}.notnull() & SokuteiDate == {date} & EkiCd == {ekicd}", engine='python')['KiroTei'].iloc[0]
            KiroTei_tail = df.query(f"EkiCdDiff{camera_num}.notnull() & SokuteiDate == {date} & EkiCd == {ekicd}", engine='python')['KiroTei'].iloc[-1]
            result_dict[date][ekicd][camera_num] = {
                'KiroTei_head': KiroTei_head,
                'KiroTei_tail': KiroTei_tail,
                'KiroTei_dist': round(KiroTei_tail - KiroTei_head, 4),
                'KiroTei_delta': round((KiroTei_tail - KiroTei_head) / (count - 1), 6),
                'count': count
            }
    return df.copy(), result_dict


def set_imgKiro(config, dir_area, df_tdm, KiroTei_dict):
    imgKilo = {}
    for camera_num in config.camera_types:
        # ã‚«ãƒ¡ãƒ©ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        image_dir = f"{config.image_dir}/{dir_area}/{camera_num}/"
        # print(image_dir)
        base_images = list_images(image_dir)
        # print(f"Image counts:{len(list_images)}")

        # è£œæ­£å¾Œã® KiroTei_delta ã‚’å‘¼ã³å‡ºã™ãŸã‚ã®ã‚­ãƒ¼ã‚’å–å¾—
        image_name = base_images[0].split('/')[-1]
        date = df_tdm[df_tdm[f'GazoFileName{camera_num}'] == image_name.split(".")[0]].iloc[0]['SokuteiDate']
        ekicd = df_tdm[df_tdm[f'GazoFileName{camera_num}'] == image_name.split(".")[0]].iloc[0]['EkiCd']
        # print(f"debug>>> {dir_area} {camera_num} >>> {date=}, {ekicd=}")

        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã«å¯¾å¿œã™ã‚‹ã‚­ãƒ­ç¨‹ã‚’æŠ½å‡ºã—ã¦è¾æ›¸å‹ã§è¨˜éŒ²ã™ã‚‹
        imgKilo_temp = {}
        # imgKilo_temp_values = {}
        # for fname in list_images:
        for idx, fname in enumerate(base_images):
        # for fname in list_images:
            image_name = re.split('[./]', fname)[-2]
            df_tdm_Series = df_tdm[df_tdm[f"GazoFileName{camera_num}"] == image_name].copy()
            if df_tdm_Series.empty:
                continue
            else:
                DenchuNo = df_tdm_Series['DenchuNo'].item()
                # KiroTei = df_tdm_Series['KiroTei'].round(4).item()
                KiroTei = KiroTei_dict[date][ekicd][camera_num]['KiroTei_head'] + KiroTei_dict[date][ekicd][camera_num]['KiroTei_delta'] * idx

            # imgKilo_temp_values["DenchuNo"] = DenchuNo
            # imgKilo_temp_values["KiroTei"] = KiroTei
            # imgKilo_temp[image_name] = imgKilo_temp_values.copy()
            imgKilo_temp[image_name] = {
                "DenchuNo": DenchuNo,
                "KiroTei": KiroTei
            }

        if imgKilo_temp != {}:
            imgKilo[camera_num] = imgKilo_temp.copy()

    return imgKilo


def get_df_tdm(config, csv_file, columns):
    """ è»Šãƒ¢ãƒ‹ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    Args:
        config: è¨­å®šãƒ‡ãƒ¼ã‚¿
        csv_file(str): S3ã«ä¿å­˜ã—ãŸç”»åƒã‚­ãƒ­ç¨‹æƒ…å ±ã‚’è¨˜éŒ²ã—ãŸCSVã®prefix
        columns(list): ç”»åƒã‚­ãƒ­ç¨‹æƒ…å ±ã®ã‚«ãƒ©ãƒ å
    Return:
        df_tdm(DataFrame): èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """

    # TDM(è»Šãƒ¢ãƒ‹CSV)ã‚’S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    download_path = f"{config.tdm_dir}/temp/{csv_file.split('/')[-1]}"
    # print("è»Šãƒ¢ãƒ‹ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ â€»å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
    download_file_from_s3(config.bucket, csv_file, download_path)
    # print(f'complete: {download_path=}')

    # CSVã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿è¾¼ã‚€
    df_tdm = pd.read_csv(download_path, names=columns, delimiter='|')

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã™ã‚‹
    df_tdm['TimeCode'] = pd.to_datetime(df_tdm['TimeCode'])
    df_tdm = df_tdm.sort_values(by=['SokuteiDate', 'KiroTei'], ignore_index=True)
    # print("è»Šãƒ¢ãƒ‹ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    # print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µã‚¤ã‚º:{df_tdm.shape}")

    # æ¬²ã—ã„åˆ—ã ã‘æŠ½å‡ºã™ã‚‹
    df_tdm = df_tdm.filter([
        'EkiCd', 'SenbetsuCd', 'SokuteiDate', 'DenchuNo', 'KiroTei',               # Comment Out    'SokuteiYear' 'NennaiSeqNo'ã¯è¿½åŠ ã•ã‚Œã¦ã„ã‚‹
        'GazoFileNameHD11', 'GazoFileNameHD12',
        'GazoFileNameHD21', 'GazoFileNameHD22',
        'GazoFileNameHD31', 'GazoFileNameHD32'
    ]).copy()

    # print("å¿…è¦ãªæƒ…å ±ã ã‘ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸ")
    # print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µã‚¤ã‚º:{df_tdm.shape}")
    return df_tdm


def get_img2kiro(config, dir_area, images_path, target_dir, base_images, csv_files):
    """ ç·šåŒºã”ã¨ã®ç”»åƒâ†’ã‚­ãƒ­ç¨‹å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»ä½œæˆã™ã‚‹
    """
    # ã‚«ãƒ©ãƒ åã‚’S3ã‹ã‚‰èª­ã¿è¾¼ã‚€
    columns_csv_key = f"{config.kiro_prefix}/{config.kiro_columns_name}"
    columns = get_columns_from_csv(config.bucket, columns_csv_key)

    # è¡Œè·¯ã®æ¡ä»¶ã‚’ãƒ•ã‚©ãƒ«ãƒ€åã‹ã‚‰æŒ‡å®šã™ã‚‹
    image_name = base_images[0].split('/')[-1]
    # st.write(f"{image_name=}")
    meas_year = image_name.split("_")[0]              # èµ°è¡Œå¹´åº¦
    meas_idx = image_name.split("_")[1]               # å¹´åº¦é€šç•ª
    meas_senku = dir_area.split('_')[0]      # ç·šåŒº
    meas_kukan = dir_area.split('_')[2]      # åŒºé–“
    # st.write(f"æ¸¬å®šå¹´åº¦: {meas_year}  å¹´åº¦é€šç•ª: {meas_idx}  ç·šåŒº: {meas_senku}  åŒºé–“: {meas_kukan}")

    # æ‰‹æŒã¡ç·šåŒºãƒ•ã‚©ãƒ«ãƒ€ã¨ä¸€è‡´ã™ã‚‹CSVã‚’S3ã‹ã‚‰æ¢ã™
    for csv_file in csv_files:
        if csv_file[-13:-4] == f"{meas_year}_{meas_idx}":
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿è¾¼ã‚€
            df_tdm = get_df_tdm(config, csv_file, columns)

            # èµ°è¡Œæ—¥ãƒ»ç·šåŒºã”ã¨ã®ã‚­ãƒ­ç¨‹è£œæ­£æƒ…å ±ã®è¾æ›¸ã‚’ä½œæˆã™ã‚‹
            df_tdm, KiroTei_dict = get_KiroTei_dict(config, df_tdm)

            # æ¸¬å®šæ—¥ã‚’å–å¾—ã€€â€»ï¼‘ã¤ã—ã‹ãªã„ã¯ãšãƒ»ãƒ»ãƒ»
            date = df_tdm['SokuteiDate'].unique().item()

            # ç·šåˆ¥ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
            if dir_area.split('_')[3] == "down":
                SenbetsuCd = 21
            elif dir_area.split('_')[3] == "up":
                SenbetsuCd = 22
            else:
                SenbetsuCd = None                              # ä»–ã¯ï¼Ÿï¼Ÿ

            # åŸºæº–ã¨ãªã‚‹é›»æŸ±ã®æƒ…å ±                                                                                               # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 
            # ref_point = {
            #     "EkiCd_NEWSS": eki_code[meas_senku][meas_kukan]["EkiCd_NEWSS"],
            #     "pole_num_NEWSS": eki_code[meas_senku][meas_kukan]["pole_num_NEWSS"],
            #     "pole_kilo_NEWSS": eki_code[meas_senku][meas_kukan]["pole_kilo_NEWSS"]
            # }
            
            # print('df_tdm <head>')
            # print(df_tdm.head())

            # pole_kiro_offset = get_Kiro_offset(df_tdm, ref_point, date)                                                        # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 

            # kiro_offset_dict = {}                                                                                              # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 
            # kiro_offset_dict[date] = pole_kiro_offset                                                                          # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 

            # # NEWSSã‚­ãƒ­ç¨‹ã‚’è¨ˆç®—ã—ã¦ åˆ—`KiroTei_NEWSS`ã«è¨˜éŒ²ã™ã‚‹                                                                  # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 
            # df_tdm = get_Kiro_NEWSS(df_tdm, kiro_offset_dict)                                                                  # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 

            # ç¢ºèªç”¨ã«ã‚¨ã‚¯ã‚»ãƒ«ã«å‡ºåŠ›ã™ã‚‹
            # df_tdm.to_excel(f"./{config.tdm_dir}/temp/df_tdm_{dir_area}.xlsx")

            # åˆæœŸè¨­å®šã®æƒ…å ±ã‚’å‡ºåŠ›
            # print(f"ç”»åƒãƒ•ã‚©ãƒ«ãƒ€åï¼š{dir_area}")
            # print(f"åŸºæº–ã«ã™ã‚‹é›»æŸ±ï¼š{ref_point['pole_num_NEWSS']}")                                                             # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 
            # print(f"ã€€ã€€ã€€ã€€ã‚­ãƒ­ç¨‹ï¼š{ref_point['pole_kilo_NEWSS']}")                                                            # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 
            # print(f"æ¤œæ¸¬ã‚­ãƒ­ç¨‹ã‚ºãƒ¬ï¼š{round(kiro_offset_dict[date], 3)}")                                                        # ã‚­ãƒ­ç¨‹ã‚ªãƒ•ã‚»ãƒƒãƒˆæœªä½¿ç”¨åŒ–ã®ãŸã‚è¿½åŠ 

            imgKilo = set_imgKiro(config, dir_area, df_tdm, KiroTei_dict)

            if imgKilo != {}:
                # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã™ã‚‹
                dir = f"{config.tdm_dir}/{dir_area}.json"
                with open(dir, mode="wt", encoding="utf-8") as f:
                    json.dump(imgKilo, f, ensure_ascii=False, indent=2, default=default)

            # st.write(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ã‚­ãƒ­ç¨‹æƒ…å ±ã‚’{dir}ã«è¨˜éŒ²ã—ã¾ã—ãŸ")
            break

    return


def download_dir(prefix, local):
    """ ãƒã‚±ãƒƒãƒˆå†…ã®æŒ‡å®šã—ãŸãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’æŒã¤ã™ã¹ã¦ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    Args:
        prefix (str): S3ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
                      (ä¾‹) imgs/Chuo_01_Tokyo-St_up_20230201_knight/
        local  (str): ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹
                      (ä¾‹) ./
    """
    # S3ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = boto3.client('s3')
    # ãƒã‚±ãƒƒãƒˆå
    bucket = 'trolley-monitor'

    paginator = client.get_paginator('list_objects')
    for result in paginator.paginate(Bucket=bucket, Prefix=prefix):
        if result.get('Contents') is not None:
            for file in result.get('Contents'):
                if not os.path.exists(os.path.dirname(local + file.get('Key'))):
                    os.makedirs(os.path.dirname(local + file.get('Key')))
                client.download_file(bucket, file.get('Key'), local + file.get('Key'))

    return


def download_file_from_s3(bucket_name, key, download_path):
    """ S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    """
    download_dir = os.path.dirname(download_path)
    if not os.path.exists(download_dir):
        os.makedirs(download_dir)

    s3 = boto3.client('s3')
    s3.download_file(bucket_name, key, download_path)


# S3ãƒã‚±ãƒƒãƒˆã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹
def put_s3_img(img, file_full_path):
    # imgã¯ã€Pillow Image.open()ã§èª­ã¿è¾¼ã¾ã‚ŒãŸå½¢å¼
    # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹
    with BytesIO() as output:
        img.save(output, format="JPEG")
        contents = output.getvalue()
    s3 = boto3.client('s3')
    response = s3.put_object(Body=contents, Bucket='k-nagayama', Key=file_full_path)
    return


def imgs_dir_remove(path):
    """ pathã§æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã™ã‚‹
    Args:
        path (str): å‰Šé™¤ã—ãŸã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    """
    try:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãã®ä¸­èº«ã‚’ã™ã¹ã¦å‰Šé™¤
        shutil.rmtree(path)
    except FileNotFoundError:
        st.error("å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print("An error occurred: ", e)
    return

def file_remove(path):
    """ pathã§æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã™ã‚‹
    Args:
        path (str): å‰Šé™¤ã—ãŸã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.remove(path)
    except FileNotFoundError:
        st.error("å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except Exception as e:
        print("An error occurred: ", e)
    return


# @st.cache
def S3_EBS_imgs_dir_Compare(S3_dir_list, EBS_dir_list, df_key):
    """ 2ã¤ã®ãƒªã‚¹ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’ã¾ã¨ã‚ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    Args:
        S3_dir_list (list): 1ã¤ç›®ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼‰S3å†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã®ãƒªã‚¹ãƒˆ
        EBS_dir_list (list): 2ã¤ç›®ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼‰EBSå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã®ãƒªã‚¹ãƒˆ
    Return:
        df (DataFrame): çµæœã‚’ã¾ã¨ã‚ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    # ç·šåŒºåãƒªã‚¹ãƒˆã®ä½œæˆï¼ˆS3_dir_listã¨EBS_dir_listã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¦ç´ ã®åˆè¨ˆï¼‰
    line_names = list(set(S3_dir_list + EBS_dir_list))
    line_names.sort()

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ
    df = pd.DataFrame(line_names, columns=['ç·šåŒºå'])

    # S3åˆ—ã¨TTSåˆ—ã®ä½œæˆ
    df['S3'] = df['ç·šåŒºå'].apply(lambda x: 'â—‹' if x in S3_dir_list else 'Ã—')
    df['TTSã‚·ã‚¹ãƒ†ãƒ '] = df['ç·šåŒºå'].apply(lambda x: 'â—‹' if x in EBS_dir_list else 'Ã—')

    if df_key:
        df_filtered = df[df['ç·šåŒºå'].str.contains(df_key)].copy()
    else:
        df_filtered = df.copy()

    return df_filtered


def rail_csv_concat(outpath):
    """ output/<camera_num>/å†…ã«ã‚ã‚‹ç”»åƒã”ã¨ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦è¿”ã™
    Args:
        outpath(str): ã‚«ãƒ¡ãƒ©æ¯ã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    Return:
        combined_df(DataFrame): å…¨ã¦ã®CSVã‚’çµåˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    csv_list = glob.glob(f"{outpath}/*.csv")
    csv_list.sort()

    if not csv_list:
        st.sidebar.warning("CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã¿ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return pd.DataFrame()

    dfs = []
    for csv_path in csv_list:
        df = pd.read_csv(csv_path)
        dfs.append(df)
    combined_df = pd.concat(dfs, ignore_index=True).sort_values(by=dfs[0].columns.tolist())

    return combined_df.copy()


def check_camera_dirs(dir_area, config):
    """ Outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹
    Args:
        dir_area (str): outputå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
        config: configãƒ•ã‚¡ã‚¤ãƒ«
    Return:
        df (DataFrame): Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼
    """
    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    result = []
    # å„ã‚«ãƒ¡ãƒ©ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    for camera_name, camera_type in config.camera_name_to_type.items():
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°ã‚’å–å¾—
        file_path = f"{config.output_dir}/{dir_area}/{camera_type}/*.csv"
        # file_path = os.path.join(config.output_dir, dir_area, camera_type, "rail.csv")
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        if glob.glob(file_path):
        # if os.path.exists(file_path):
            result.append([f"{camera_name}_{camera_type}", "â—‹"])
        else:
            result.append([f"{camera_name}_{camera_type}", "Ã—"])
    # çµæœã‚’Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    df = pd.DataFrame(result, columns=["ã‚«ãƒ¡ãƒ©ç•ªå·", "çµæœæœ‰ç„¡"])
    return df

def check_camera_dirs_addIdxLen(dir_area, config):
    """ Outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹
    Args:
        dir_area (str): outputå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
        config: configãƒ•ã‚¡ã‚¤ãƒ«
    Return:
        df (DataFrame): Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼
    """
    result = []
    # å„ã‚«ãƒ¡ãƒ©ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    with ThreadPoolExecutor(max_workers=6) as executor:
        for camera_name, camera_type in config.camera_name_to_type.items():
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’ä½œæˆ
            # file_path = os.path.join(config.output_dir, dir_area, camera_type, "rail.csv")
            file_path = f"{config.output_dir}/{dir_area}/{camera_type}"
            res = executor.submit(read_csv_idx, camera_name, camera_type, file_path)
            result.append(res.result())
    # çµæœã‚’Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    df = pd.DataFrame(result, columns=["ã‚«ãƒ¡ãƒ©ç•ªå·", "çµæœæœ‰ç„¡", "æœ€å¾Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"])
    return df


def get_max_idx(file_path):
    """ check_camera_dirs_addIdxLenç”¨ã®é–¢æ•°
    Args:
        file_path(str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
    Return:
        max_val(int): image_idxã®æœ€å¤§å€¤
    """
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®image_idxã®æœ€å¤§å€¤ã‚’å–å¾—ã™ã‚‹
    flist = glob.glob(f"{file_path}/*.csv")
    flist.sort()
    if not flist:
        return -1    # CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã„ã¨ãã¯return
    max_val = float('-inf')
    # ä¸€ç•ªæœ«å°¾ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ãŠã„ã¦ã€æœ€çµ‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
    with open(flist[-1], 'r', encoding="utf-8") as file:
        # Skip header
        file.readline()
        for line in file:
            try:
                value = float(line.split(',', 1)[0])  # Split only at the first comma
                if value > max_val:
                    max_val = value
            except ValueError:
                continue

    # ç©ºã®rail.csvã®å ´åˆã®å¯¾å‡¦
    # max_valãŒæœ‰é™ã®å€¤ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    if np.isfinite(max_val):
        return int(max_val)
    else:
        return -1    # å¾Œã§ +1 ã•ã‚Œã‚‹ãŸã‚ -1ã«ã—ã¦ãŠã

def read_csv_idx(camera_name, camera_type, file_path):
    """ check_camera_dirs_addIdxLenç”¨ã®é–¢æ•°
    Args:
        ile_path(str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
    Return:
        result_list(list): çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ è¿½è¨˜ç”¨ã®ãƒªã‚¹ãƒˆ
    """
    # if os.path.exists(file_path):
    if glob.glob(file_path):
        max_idx = get_max_idx(file_path)
        result_list = [f"{camera_name}_{camera_type}", "â—‹", int(max_idx) + 1]    # ãƒ¦ãƒ¼ã‚¶å‘ã‘ã«+1è¡¨ç¤ºã™ã‚‹
    else:
        result_list = [f"{camera_name}_{camera_type}", "Ã—", 0]
    return result_list


def check_camera_results(dir_area, config):
    """ Outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ï¼‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹
    Args:
        dir_area (str): outputå†…ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
        config: configãƒ•ã‚¡ã‚¤ãƒ«
    Return:
        df (DataFrame): Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼
    """
    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    result = []

    # å„ã‚«ãƒ¡ãƒ©ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    for camera_name, camera_type in config.camera_name_to_type.items():
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’ä½œæˆ
        dir_path = os.path.join(config.output_dir, dir_area, camera_type)
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        try:
            shelve_check = False
            csv_check = False
            for file in os.listdir(dir_path):
                if "rail.shelve" in file:  # ãƒ•ã‚¡ã‚¤ãƒ«åã«"rail.shelve"ãŒå«ã¾ã‚Œã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                    shelve_check = True
                if file.endswith(".csv"):  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒCSVå½¢å¼ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                    csv_check = True
                if shelve_check and csv_check:  # ä¸¡æ–¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    break
            result.append([f"{camera_name}_{camera_type}", "â—‹" if shelve_check else "Ã—", "â—‹" if csv_check else "Ã—"])
        except FileNotFoundError:  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆ
            result.append([f"{camera_name}_{camera_type}", "Ã—", "Ã—"])

    # çµæœã‚’Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    df = pd.DataFrame(result, columns=["ã‚«ãƒ¡ãƒ©ç•ªå·", "çµæœæœ‰ç„¡", "CSVå¤‰æ›"])

    return df


# def trim_trolley_dict(config, trolley_dict, img_path):
#     """ shelveã‹ã‚‰èª­ã¿å–ã£ãŸtrolley_dictã®è¡Œæ•°ã‚’æƒãˆã‚‹
#     Args:
#         config: è¨­å®šç”¨ãƒ•ã‚¡ã‚¤ãƒ«
#         trolley_dict(dict): shleveã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¾æ›¸
#         img_path(str): è§£æå¯¾è±¡ã®ç”»åƒãƒ‘ã‚¹
#     Return:
#         trolley_dict(dict): æ›´æ–°ã•ã‚ŒãŸtrolley_dict
#     Memo:
#         ãƒ‡ãƒãƒƒã‚°ç”¨ã®printæ–‡ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã„ã¾ã™ã€‚
#         å¿…è¦ãªå ´åˆã¯æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„ã€‚
#     """
#     for trolley_id in config.trolley_ids[:len(trolley_dict[img_path])]:
#         # print(trolley_id)
#         for key in trolley_dict[img_path][trolley_id].keys():
#             if not trolley_dict[img_path][trolley_id][key]:
#                 # ç©ºã®ãƒªã‚¹ãƒˆã®å ´åˆ
#                 trolley_dict[img_path][trolley_id][key] = [np.nan] * config.max_len
#             elif not isinstance(trolley_dict[img_path][trolley_id][key], list):
#                 # ãƒªã‚¹ãƒˆä»¥å¤–(æ•°å€¤ç­‰)ã®å ´åˆ
#                 trolley_dict[img_path][trolley_id][key] = [trolley_dict[img_path][trolley_id][key]] + [np.nan] * (config.max_len - 1)
#             value_len = len(trolley_dict[img_path][trolley_id][key])
#             if config.max_len < value_len:
#                 config.max_len = value_len
#             if value_len < config.max_len:
#                 # è¦ç´ ãŒmax_len(ä¾‹:1000è¡Œ)æœªæº€ã®å ´åˆ
#                 if key == "ix":
#                     # é€£ç•ªã§åŸ‹ã‚ã‚‹
#                     trolley_dict[img_path][trolley_id][key].extend(range(value_len, config.max_len))
#                 else:
#                     # NaNåŸ‹ã‚ã™ã‚‹
#                     trolley_dict[img_path][trolley_id][key].extend([np.nan] * (config.max_len - value_len))
#     return trolley_dict


# def read_trolley_dict(config, trolley_dict, img_idx, img_path, column_name, thin_out):
#     """ ç”»åƒãƒ‘ã‚¹ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
#     Args:
#         config: è¨­å®šç”¨ãƒ•ã‚¡ã‚¤ãƒ«
#         trolley_dict(dict): shleveã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¾æ›¸
#         img_path(str): è§£æå¯¾è±¡ã®ç”»åƒãƒ‘ã‚¹
#         thin_out(int): è¡Œã‚’é–“å¼•ãé–“éš”(ä¾‹)50 â‡’ æ¨ª50pxãšã¤ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã™ã‚‹
#     Return:
#         df_trolley(DataFrame): trolley_dictã‹ã‚‰ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
#     """
#     # Step0: trolley_dictã®è¡Œæ•°ã‚’æƒãˆã‚‹
#     trim_trolley_dict(config, trolley_dict, img_path)

#     # Step1: trolley_dictã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦èª­ã¿è¾¼ã‚€
#     df_trolley = pd.DataFrame()
#     # trolley_idã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦èª­ã¿è¾¼ã‚€
#     for idx, trolley_id in enumerate(config.trolley_ids):
#         if trolley_id in trolley_dict[img_path]:
#             # trolley_idã«å¯¾å¿œã™ã‚‹çµæœãŒã‚ã‚‹å ´åˆ
#             # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿è¾¼ã‚€
#             df = pd.DataFrame(trolley_dict[img_path][trolley_id]).copy()
#             # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨result_keysã‚’æ¯”è¼ƒã—ã¦ã€ä¸è¶³ã™ã‚‹åˆ—ã‚’è¿½åŠ ã™ã‚‹
#             diff_list = list(set(config.result_keys) - set(list(df.columns)))
#             for column in diff_list:
#                 df[column] = pd.Series([np.nan]*config.max_len, index=df.index)
#             # result_keysã¨åŒã˜é †ç•ªã«ä¸¦ã³æ›¿ãˆã‚‹
#             df = df.reindex(columns=config.result_keys, copy=False)
#         else:
#             # trolley_idã«å¯¾å¿œã™ã‚‹çµæœãŒç„¡ã„å ´åˆ
#             # ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã™ã‚‹
#             df_forNoneTrolley = pd.DataFrame(index=range(config.max_len), columns=config.result_keys)

#         # dfã¨df_trolleyã‚’é€£çµã™ã‚‹
#         if not idx:
#             df_trolley = df.copy()
#         else:
#             df_trolley = pd.concat([df_trolley, df], axis=1).copy()

#     # Step2: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—åã‚’å¤‰æ›´ã™ã‚‹
#     df_trolley.columns = column_name

#     # Step3: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚„ç”»åƒåç­‰ã®æƒ…å ±ã‚’è¨˜éŒ²ã™ã‚‹
#     # ç·šåŒºåã€ã‚«ãƒ¡ãƒ©åã€ç”»åƒåã‚’å–å¾—
#     dir_area, camera_num = img_path.split("/")[1:3]
#     image_name = img_path.split('/')[-1]

#     # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æŒ¿å…¥ã™ã‚‹é …ç›®ãƒ»ä½ç½®ã‚’æŒ‡å®š
#     columns_to_insert = [("img_idx", img_idx, 0),
#                          ("dir_area", dir_area, 1),
#                          ("camera_num", camera_num, 2),
#                          ("image_name", image_name, 3)]

#     # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ç”»åƒåç­‰ã‚’æŒ¿å…¥ã™ã‚‹
#     for col_name, col_value, idx in columns_to_insert:
#         if col_name not in df_trolley.columns:
#             df_trolley.insert(idx, col_name, col_value)
#         else:
#             df_trolley[col_name] = col_value

#     # Step4: ãƒ‡ãƒ¼ã‚¿ã‚’é–“å¼•ã
#     # df_trolley = df_trolley[::thin_out]

#     return df_trolley


# def trolley_dict_fillna(img_idx, img_path, dfs_columns):
#     """
#     Args:
#         img_idx (int): ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
#         img_path (str): ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
#         dfs_columns(DataFrame columns): çµåˆå…ˆã®åˆ—å
#     Return:
#         df_trolley(DataFrame): NaNåŸ‹ã‚ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
#     """
#     df_trolley = pd.DataFrame(columns=dfs_columns)
#     df_trolley['ix'] = range(img_idx * 1000, img_idx * 1000 + 1000)
#     df_trolley.fillna(np.nan)
#     dir_area, camera_num = img_path.split("/")[1:3]
#     image_name = img_path.split('/')[-1]
#     # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æŒ¿å…¥ã™ã‚‹é …ç›®ãƒ»ä½ç½®ã‚’æŒ‡å®š
#     columns_to_insert = [("img_idx", img_idx, 0),
#                          ("dir_area", dir_area, 1),
#                          ("camera_num", camera_num, 2),
#                          ("image_name", image_name, 3)]

#     # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ç”»åƒåç­‰ã‚’æŒ¿å…¥ã™ã‚‹
#     for col_name, col_value, idx in columns_to_insert:
#         if col_name not in df_trolley.columns:
#             df_trolley.insert(idx, col_name, col_value)
#         else:
#             df_trolley[col_name] = col_value
    
#     return df_trolley

# @my_logger
def result_csv_load(config, rail_fpath):
    """ çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    Args:
        config(instance): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        rail_fpath(str) : çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
    Return:
        df_csv(DataFrame): CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    # çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€
    # ç„¡ã‘ã‚Œã°ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã™ã‚‹
    if os.path.exists(rail_fpath):
        df_csv = pd.read_csv(rail_fpath)
    else:
        df_csv = pd.DataFrame(columns=config.columns_list)
    return df_csv


# @my_logger
def result_csv_crop(df_csv, dir_area, camera_num, image_name, trolley_id):
    """ è§£æå¯¾è±¡ç”»åƒã®çµæœã ã‘ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡Œã‚’çµã‚Šè¾¼ã‚€
    Args:
        df_csv(DataFrame): å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        dir_area(str)    : ç·šåŒºå
        camera_num(str)  : ã‚«ãƒ¡ãƒ©ç•ªå·
        image_name(str)  : ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ä»˜ãï¼‰
        trolley_id(str)  : ãƒˆãƒ­ãƒªãƒ¼ID
    Return:
        df_csv_crop(DataFrame): çµè¾¼ã¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    condition = (
        (df_csv['measurement_area'] == dir_area) &
        (df_csv['camera_num'] == camera_num) &
        (df_csv['image_name'] == image_name) &
        (df_csv['trolley_id'] == trolley_id)
    )
    # conditionã§æŒ‡å®šã—ãŸæ¡ä»¶ã«åˆã†è¡Œã ã‘ã‚’æŠ½å‡ºã—ã¦ã‚³ãƒ”ãƒ¼
    return df_csv.loc[condition, :].copy()


def result_csv_drop(rail_fpath, dir_area, camera_num, image_name, trolley_id, config):
    """ ç”»åƒ1æšãƒ»ã‚«ãƒ¡ãƒ©ç•ªå·ãƒ»TrolleyIDå˜ä½ã§çµæœã‚’å‰Šé™¤ã™ã‚‹
    Args:
        rail_fpath(str)  : çµæœCSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        dir_area(str)    : ç·šåŒºå
        camera_num(str)  : ã‚«ãƒ¡ãƒ©ç•ªå·
        image_name(str)  : ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ä»˜ãï¼‰
        trolley_id(str)  : ãƒˆãƒ­ãƒªãƒ¼ID
        config(instance) : è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    Return:
        None
    """
    df_csv = result_csv_load(config, rail_fpath)
    condition = (
        (df_csv['measurement_area'] == dir_area) &
        (df_csv['camera_num'] == camera_num) &
        (df_csv['image_name'] == image_name) &
        (df_csv['trolley_id'] == trolley_id)
    )
    df_csv.drop(df_csv[condition].index).to_csv(rail_fpath, index = False)
    return None


# @my_logger
def result_dict_to_csv(config, result_dict, idx, count, dir_area, camera_num, image_name, trolley_id, ix_list):
    """ è§£æå¾Œã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ä½œæˆã—ãŸè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
    Args:
        config(instance) : è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        df_csv(DataFrame): çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        rail_fpath(str)  : çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
        result_dict(dict): ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ç”Ÿæˆã—ãŸè¾æ›¸
        idx(int)         : ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        image_path(str)  : ç”»åƒãƒ‘ã‚¹
        image_name(str)  : ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å
        trolley_id(str)  : ãƒˆãƒ­ãƒªãƒ¼ID
        window(int)      : æ¨™æº–åå·®ã‚’è¨ˆç®—ã™ã‚‹ã¨ãã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
    Return:
        df_csv(DataFrame): çµæœã‚’æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å†…å®¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    df = pd.DataFrame.from_dict(result_dict[trolley_id], orient='index').T

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ç”»åƒåç­‰ã‚’æŒ¿å…¥
    df.insert(0, 'image_idx', idx + count - 1)
    df.insert(1, 'ix', ix_list[:len(df)])
    df['ix'] = df['ix'] + (idx + count - 1) * 1000
    df.insert(2, 'measurement_area', dir_area)
    df.insert(3, 'camera_num', camera_num)
    df.insert(4, 'image_name', image_name)
    df.insert(5, 'trolley_id', trolley_id)
    df.insert([df.columns.get_loc(c) for c in df.columns if 'estimated_lower_edge' in c][0] + 1,
              'estimated_width', df['estimated_lower_edge'] - df['estimated_upper_edge'])

    # ä¸è¶³ã™ã‚‹åˆ—ã‚’è¿½åŠ ã™ã‚‹
    for i, col in enumerate(config.columns_list):
        if col not in df.columns:
            df.insert(i, col, pd.NA)

    return df

def experimental_result_dict_to_csv(config, result_dict, kiro_dict, kiro_init_dict, idx, count, dir_area, camera_num, image_name, trolley_id, x_init, ix_list):
    """ è§£æå¾Œã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ä½œæˆã—ãŸè¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
        â€»é«˜å´æ¤œè¨¼ç”¨ ä¸€éƒ¨ç·šåŒºã§ã®ã¿ä½¿ç”¨å¯èƒ½
    Args:
        config(instance) : è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        result_dict(dict): ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ç”Ÿæˆã—ãŸè¾æ›¸
        kiro_dict(dict)  : ç”»åƒã”ã¨ã®ã‚­ãƒ­ç¨‹æƒ…å ±ã®è¾æ›¸
        kiro_init_dict(dict): ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åãŒkiro_dictã«å«ã¾ã‚Œã‚‹ç¯„å›²ã®æƒ…å ±ã‚’è¨˜éŒ²ã—ãŸè¾æ›¸
        idx(int)         : ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        image_path(str)  : ç”»åƒãƒ‘ã‚¹
        image_name(str)  : ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å
        trolley_id(str)  : ãƒˆãƒ­ãƒªãƒ¼ID
        window(int)      : æ¨™æº–åå·®ã‚’è¨ˆç®—ã™ã‚‹ã¨ãã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        x_init(int)      : åˆ†æã‚’é–‹å§‹ã—ãŸxåº§æ¨™
        ix_list(list)    : ixå…¥åŠ›ç”¨ã®ãƒªã‚¹ãƒˆ
    Return:
        df_csv(DataFrame): çµæœã‚’æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚­ãƒ­ç¨‹ã‚’ç´ã¥ã‘ã‚‹ãŸã‚ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¾æ›¸ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    # with open(f"{config.tdm_dir}/{dir_area}.json", 'r') as file:
    #     kiro_dict = json.load(file)

    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å†…å®¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    df = pd.DataFrame.from_dict(result_dict[trolley_id], orient='index').T
    # df.to_csv("temp.csv", index=False)    # ãƒ‡ãƒãƒƒã‚°ç”¨

    # ãƒ‡ãƒãƒƒã‚°ç”¨
    # ---------------------------------------
    # st.write(ix_list[x_init:(x_init+len(df))])
    # st.write(f"x_init = {x_init}")
    # st.write(f"len(df) = {len(df)}")
    # st.write(f"x_init+len(df) = {x_init+len(df)}")
    # st.write(f"len(ix_list[x_init:(x_init+len(df))]) = {len(ix_list[x_init:(x_init+len(df))])}")
    # ---------------------------------------

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ç”»åƒåç­‰ã‚’æŒ¿å…¥
    df.insert(0, 'image_idx', idx + count - 1)
    # st.write(df)    # ãƒ‡ãƒãƒƒã‚°ç”¨
    if count == 1:
        df.insert(1, 'ix', ix_list[x_init:(x_init + len(df))])
    else:
        df.insert(1, 'ix', ix_list[:len(df)])
    df['ix'] = df['ix'] + (idx + count - 1) * 1000
    fname = image_name.split(".")[0]

    # -----------------------------------
    """
    ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åãŒãƒãƒƒãƒã—ãªã„ç¯„å›²ã®ã‚­ãƒ­ç¨‹ã‚’è¨ˆç®—ã—ã¦è¨˜éŒ²ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ä¸­
    """
    # -----------------------------------
    if kiro_dict:
        # ã‚­ãƒ­ç¨‹ã®å¢ƒç•Œæ¡ä»¶ã‚’å–å¾—
        kiro_tei_init_head = kiro_init_dict['KiroTei_init'][0]
        kiro_tei_init_tail = kiro_init_dict['KiroTei_init'][1]
        if fname in kiro_dict[camera_num].keys():
            DenchuNo = kiro_dict[camera_num][fname]['DenchuNo']
            kiro_tei = kiro_dict[camera_num][fname]['KiroTei']
            # st.write(f"Match   > kiro_tei: {kiro_tei}")
        else:
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åãŒãƒãƒƒãƒã—ãªã„å ´åˆã®ã‚­ãƒ­ç¨‹ã‚’æŒ‡å®š
            if idx + count - 1 <= kiro_init_dict['image_idx_init'][0]:
                DenchuNo = 0
                kiro_tei = kiro_tei_init_head - (kiro_init_dict['image_idx_init'][0] - (idx + count - 1)) * 2 / config.img_width
            else:
                DenchuNo = 1000
                kiro_tei = kiro_tei_init_tail + ((idx + count - 1) - kiro_init_dict['image_idx_init'][1]) * 2 / config.img_width
        kiro_tei_list = [kiro_tei + ix / config.img_width / 1000 * 2 for ix in config.ix_list]
        df.insert(2, 'pole_num', DenchuNo)
        if count == 1:
            df.insert(3, 'kiro_tei', kiro_tei_list[x_init:(x_init+len(df))])
        else:
            df.insert(3, 'kiro_tei', kiro_tei_list[:len(df)])
    else:
        df.insert(2, 'pole_num', "Empty")
        # if count == 1:
        #     df.insert(3, 'kiro_tei', ix_list[x_init:(x_init + len(df))])
        # else:
        #     df.insert(3, 'kiro_tei', ix_list[:len(df)])
        df.insert(3, 'kiro_tei', df['ix'] / 500000)

    df.insert(4, 'measurement_area', dir_area)
    df.insert(5, 'camera_num', camera_num)
    df.insert(6, 'image_name', image_name)
    df.insert(7, 'trolley_id', trolley_id)
    df.insert([df.columns.get_loc(c) for c in df.columns if 'estimated_lower_edge' in c][0] + 1,
              'estimated_width', df['estimated_lower_edge'] - df['estimated_upper_edge'])

    # ä¸è¶³ã™ã‚‹åˆ—ã‚’è¿½åŠ ã™ã‚‹
    for i, col in enumerate(config.columns_list):
        if col not in df.columns:
            df.insert(i, col, pd.NA)

    return df

def experimental_get_image_match(list_images, kiro_dict, camera_num):
    """ ãƒ­ãƒ¼ã‚«ãƒ«ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã€ã‚­ãƒ­ç¨‹æƒ…å ±ã®è¾æ›¸ã«å«ã¾ã‚Œã‚‹ç¯„å›²ã‚’å–å¾—ã™ã‚‹
    Args:
        list_images(list): ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
        kiro_dict(dict): è»Šãƒ¢ãƒ‹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä½œæˆã—ãŸã‚­ãƒ­ç¨‹æƒ…å ±ã®è¾æ›¸
        camera_num(str): è§£æä¸­ã®ã‚«ãƒ¡ãƒ©ç•ªå· (ä¾‹)HD11
    Return:
        kiro_init_dict(dict): ãƒãƒƒãƒã—ãŸæƒ…å ±ã®è¾æ›¸
            keys:
                image_name_init(list): ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å[é–‹å§‹, çµ‚äº†]
                image_idx_init(list): ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹[é–‹å§‹, çµ‚äº†]
                DenchuNo_init(list): é›»æŸ±ç•ªå·[é–‹å§‹, çµ‚äº†]
                KiroTei_init(list): ã‚­ãƒ­ç¨‹[é–‹å§‹, çµ‚äº†]
    """
    find_kiro_idx_head = 0
    find_kiro_idx_tail = 0
    find_image_name_head = ""
    find_image_name_tail = ""
    kiro_keys = set(kiro_dict[camera_num].keys())  # è¾æ›¸ã®ã‚­ãƒ¼ã‚’ã‚»ãƒƒãƒˆã«å¤‰æ›

    # find_kiro_idx_headã‚’è¦‹ã¤ã‘ã‚‹
    for idx, fname in enumerate(list_images):
        image_name = re.split('[./]', fname)[-2]
        if image_name.split(".")[0] in kiro_keys:
            find_kiro_idx_head = idx
            find_image_name_head = image_name
            break

    # find_kiro_idx_tailã‚’è¦‹ã¤ã‘ã‚‹
    for idx, fname in reversed(list(enumerate(list_images))):
        image_name = re.split('[./]', fname)[-2]
        if image_name.split(".")[0] in kiro_keys:
            find_kiro_idx_tail = idx
            find_image_name_tail = image_name
            break

    # find_kiro_idx_headãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®å‡¦ç†
    if not find_kiro_idx_head:
        find_kiro_idx_tail = len(list_images) - 1

    kiro_init_dict = {
        'image_name_init': [find_image_name_head, find_image_name_tail],
        'image_idx_init': [find_kiro_idx_head, find_kiro_idx_tail],
        'DenchuNo_init': [
            kiro_dict[camera_num][find_image_name_head]['DenchuNo'],
            kiro_dict[camera_num][find_image_name_tail]['DenchuNo']
        ],
        'KiroTei_init': [
            kiro_dict[camera_num][find_image_name_head]['KiroTei'],
            kiro_dict[camera_num][find_image_name_tail]['KiroTei']
        ],
    }
    return kiro_init_dict


# @my_logger
def dfcsv_update(config, df_csv, df):
    """ è§£æçµæœdfã®å†…å®¹ã‚’df_csvã«è¿½è¨˜/æ›´æ–°ã™ã‚‹
    Args:
        df_csv(DataFrame): çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        df(DataFrame): ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰å¤‰æ›ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆç”»åƒã”ã¨ï¼‰
        x_init(int): è§£æé–‹å§‹æ™‚ã®xåº§æ¨™
        condition(Pandas Series): æŒ‡å®šæ¡ä»¶ã¸ã®ä¸€è‡´çŠ¶æ…‹ã‚’è¨˜éŒ²ã—ãŸå¤‰æ•°ï¼ˆç”»åƒã”ã¨ï¼‰
        count(int): è§£æé–‹å§‹ã‹ã‚‰ã®ç”»åƒã‚«ã‚¦ãƒ³ãƒˆï¼ˆ1æšç›®ã¯count=1ï¼‰
    Return:
        merged(DataFrame): çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã®ãŸã‚ã®ã‚­ãƒ¼ã‚’å®šç¾©
    grouping_keys = ['measurement_area', 'camera_num', 'image_name', 'trolley_id', 'ix']

    # æŒ‡å®šã®ã‚­ãƒ¼ã«åŸºã¥ã„ã¦dfã‚’df_csvã«ãƒãƒ¼ã‚¸
    merged = pd.merge(df_csv, df, on=['measurement_area', 'camera_num', 'image_name', 'trolley_id', 'ix'], 
                      how='outer', suffixes=('', '_new'))

    # ä¸€è‡´ã™ã‚‹è¡ŒãŒã‚ã‚Œã°ã€dfã®å€¤ã§df_csvã®å€¤ã‚’ä¸Šæ›¸ã
    for col in df.columns:
        if col not in grouping_keys:
            if col in merged.columns and (col + '_new') in merged.columns:
                # 2å›ç›®ä»¥é™ã®å®Ÿè¡Œç”¨: colã¨col+'_new'ãŒã‚«ãƒ©ãƒ åã«ã‚ã‚‹å ´åˆã ã‘å‡¦ç†ã™ã‚‹
                merged[col] = merged[col + '_new'].combine_first(merged[col])
                merged.drop(col + '_new', axis=1, inplace=True)

    # df_csv(merged)ã®ã‚«ãƒ©ãƒ ã®é †ç•ªã‚’åˆã‚ã›ã‚‹
    merged = merged[config.columns_list]

    return merged

# @my_logger
def window2min_periods(window):
    """ æ¨™æº–åå·®ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‹ã‚‰min_periodsã‚’è¨­å®šã™ã‚‹
    Args:
        window(int): ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
    Return:
        min_periods(int): æœ€å°è¨ˆç®—ç¯„å›²
    """
    if window <= 2:
        window = 2
        min_periods = 1
    elif window > 2:
        min_periods = int(window / 2)
    return min_periods

def dfcsv_std_calc(df_csv, col_name, col_name_std, window, min_periods, col_name_ref):
    """ df_csvã«ãŠã‘ã‚‹widthã®æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦df_csvã«è¿½è¨˜ã™ã‚‹
    Args:
        df_csv(DataFrame): çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        col_name(str): è¨ˆç®—å¯¾è±¡ã®åˆ—å
        col_name_std(str): æ¨™æº–åå·®ã‚’è¨˜éŒ²ã™ã‚‹åˆ—å
        window(int): ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        min_periods(int): æœ€å°è¨ˆç®—ç¯„å›²
        col_name_ref(str): NaNãŒå«ã¾ã‚Œã‚‹è¡Œã‚’é™¤å¤–ã™ã‚‹éš›ã«ç¢ºèªå¯¾è±¡ã«ã™ã‚‹åˆ—å
    Return:
        df_csv(DataFrame): æ¨™æº–åå·®ã‚’è¿½è¨˜ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    # estimated_upper_edgeãŒNaNã§ãªã„è¡Œã ã‘é¸æŠã—ã¦æ¨™æº–åå·®ã‚’è¨ˆç®—
    non_nan_rows = df_csv[col_name_ref].notna()
    df_csv.loc[non_nan_rows, col_name_std] = df_csv.loc[non_nan_rows, col_name].rolling(window=window, min_periods=min_periods).std()
    return df_csv

# @my_logger
def trolley_dict_to_csv(config, rail_fpath, camera_num, base_images, window, log_view, progress_bar):
    """ Shelveãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    Args:
        config: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        rail_fpath (str): shelveãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
        camera_num (str): é¸æŠã•ã‚ŒãŸã‚«ãƒ¡ãƒ©ç•ªå·
        base_images (list): ã‚«ãƒ¡ãƒ©ç•ªå·ã«å¯¾å¿œã™ã‚‹ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        thin_out (int): CSVã®è¡Œã‚’é–“å¼•ãé–“éš” (ä¾‹)50 â‡’ æ¨ª50pxãšã¤ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã™ã‚‹
        window (int): æ¨™æº–åå·®ã®è¨ˆç®—ã«ç”¨ã„ã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        log_view(st.empty): Streamlitã®ã‚³ãƒ³ãƒ†ãƒŠï¼ˆãƒ­ã‚°è¡¨ç¤ºç”¨ã®ã‚¨ãƒªã‚¢ã‚’æŒ‡å®šï¼‰
        progress_bar(st.progress): Streamlitã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    """
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹ã‚’æŒ‡å®š
    csv_fpath = rail_fpath.replace(".shelve", ".csv")
    log_view.write(f"csv_fpath:{csv_fpath}")
    
    # æ¨™æº–åå·®è¨ˆç®—ç”¨>ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æœ€å°è¨ˆç®—å˜ä½ã‚’æŒ‡å®š
    # min_periodsã®æŒ‡å®šå€¤ä»¥ä¸‹ã§æ¨™æº–åå·®ã‚’è¨ˆç®—ã™ã‚‹å ´åˆã¯NaNã«ãªã‚‹
    if window <= 2:
        window = 2
        min_periods = 1
    elif window > 2:
        min_periods = int(window / 2)
    # st.sidebar.write(f"window:{window}")
    # st.sidebar.write(f"min_periods:{min_periods}")

    # shelveãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    with shelve.open(rail_fpath, flag='r', encoding="utf-8") as rail:
        # trolley_dict = copy.deepcopy(rail[camera_num])
        trolley_dict = rail[camera_num]

    df_concat = pd.DataFrame(columns=config.columns_list)
    for idx, image_path in enumerate(trolley_dict.keys()):
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
        progress_bar.progress(idx / len(base_images))

        # ç·šåŒºåç­‰ã®æƒ…å ±ã‚’å–å¾—
        dir_area, camera_num = image_path.split("/")[1:3]
        image_name = image_path.split('/')[-1]

        # shelveã®ä¸­èº«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚³ãƒ”ãƒ¼
        for trolleyid in trolley_dict[image_path].keys():
            # print(f'trolley ID> {trolleyid}')
            df = pd.DataFrame.from_dict(trolley_dict[image_path][trolleyid], orient='index').T
            # <ãƒ‡ãƒ¼ã‚¿æ•´å½¢>ãƒ‡ãƒ¼ã‚¿æ•°ãŒ1000è¡Œæœªæº€ã®å ´åˆã«ç©ºç™½è¡Œã‚’è¿½åŠ ã™ã‚‹
            if len(df) < 1000:
                num_rows_to_add = config.max_len - df.shape[0]
                if num_rows_to_add > 0:
                    empty_df = pd.DataFrame(np.nan, index=range(num_rows_to_add), columns=df.columns)
                    df = pd.concat([df, empty_df], ignore_index=True)
                    
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ç”»åƒåç­‰ã‚’æŒ¿å…¥
            df.insert(0, 'image_idx', idx)
            df.insert(1, 'ix', config.ix_list)
            df['ix'] = df['ix'] + idx * 1000
            df.insert(2, 'measurement_area', dir_area)
            df.insert(3, 'camera_num', camera_num)
            df.insert(4, 'image_name', image_name)
            df.insert(5, 'trolley_id', trolleyid)
            df.insert([df.columns.get_loc(c) for c in df.columns if 'estimated_lower_edge' in c][0] + 1,
                      'estimated_width', df['estimated_lower_edge'] - df['estimated_upper_edge'])

            # df['ix'] = ix_list
            # df['ix'] = df['ix'] + idx * 1000
            # df['estimated_width'] = df['estimated_lower_edge'] - df['estimated_upper_edge']
            # df['image_path'] = image_path
            # df['trolley_id'] = trolleyid
            try:
                df_concat = pd.concat([df_concat, df], ignore_index=True)
            except:
                print("concat error -> skip")

    # # estimated_widthã®æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦è¨˜éŒ²ã™ã‚‹
    df_concat['estimated_width_std'] = df_concat['estimated_width'].rolling(window=window, min_periods=min_periods).std()

    # print(df_concat.shape)
    df_concat.to_csv(csv_fpath, encoding='cp932')
    # print(f'csv file convert -> {csv_fpath}')

    # ğŸ‘‡ä»¥å‰ã®ã‚³ãƒ¼ãƒ‰
#     # èª­ã¿è¾¼ã‚“ã è¾æ›¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
#     dfs = pd.DataFrame()
#     # CSVã®åˆ—åã‚’æº–å‚™ã™ã‚‹
#     column_name = [
#         f"{trolley_id}_{key}"
#         for trolley_id in config.trolley_ids
#         for key in config.result_keys
#     ]
#     # trolley_dictã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
#     for img_idx, img_path in enumerate(base_images):
#         # å¤‰æ›ã®é€²æ—ã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«è¡¨ç¤ºã™ã‚‹
#         progress_bar.progress(img_idx / len(base_images))
#         # img_pathã®çµæœãŒç©ºã§ãªã„ã¨ãã«å®Ÿè¡Œã™ã‚‹
#         # log_view.write(f"{img_idx}> img_path: {img_path}")
#         if len(trolley_dict[img_path]):
#             # è§£æçµæœãŒã‚ã‚‹ã¨ã
#             df_trolley = read_trolley_dict(config, trolley_dict, img_idx, img_path, column_name, thin_out).copy()
#         elif img_idx:
#             # img_pathã®çµæœãŒç©ºã®ã¨ã & img_idxãŒ0ä»¥å¤–ã®ã¨ã
#             df_trolley = trolley_dict_fillna(img_idx, img_path, dfs.columns).copy()
#         elif img_idx:
#             # 1æšç›®ã®ç”»åƒã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
#             print("è§£æçµæœãŒãªã„ãŸã‚ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
#             break
#         # df_trolleyã‚’çµåˆã™ã‚‹
#         dfs = pd.concat([dfs, df_trolley], ignore_index=True)

#     # estimated_widthã®æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦è¿½è¨˜ã™ã‚‹
#     dfs = width_std_calc(config, dfs, window)

#     # ãƒ‡ãƒ¼ã‚¿ã‚’é–“å¼•ã
#     dfs = dfs[::thin_out]

#     # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¦outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ä¿å­˜ã™ã‚‹
#     dfs.to_csv(csv_fpath, encoding='cp932')

    return

# def width_std_calc(config, dfs, window):
#     """ estimated_widthã®æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦è¿½è¨˜ã™ã‚‹
#     Args:
#         config(dict)    : è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
#         dfs(DataFrame)  : è¨ˆç®—å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
#         window(int)   : ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚µã‚¤ã‚º
#     Return:
#         dfs(DataFrame)  : æ¨™æº–åå·®ã‚’è¿½è¨˜ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
#     """
#     # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æœ€å°è¨ˆç®—å˜ä½ã‚’æŒ‡å®š
#     # min_periodsã®æŒ‡å®šå€¤ä»¥ä¸‹ã§æ¨™æº–åå·®ã‚’è¨ˆç®—ã™ã‚‹å ´åˆã¯NaNã«ãªã‚‹
#     if window <= 2:
#         window = 2
#         min_periods = 1
#     elif window > 2:
#         min_periods = int(window / 2)
#     # st.sidebar.write(f"window:{window}")
#     # st.sidebar.write(f"min_periods:{min_periods}")

#     # estimated_widthã®åˆ—ã‚’å–å¾—
#     insert_positions = [
#         dfs.columns.get_loc(c) for c in dfs.columns if 'estimated_width' in c
#     ]

#     # æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦widthã®å³éš£ã‚Šã«è¿½è¨˜ã™ã‚‹
#     for trolley_id, insert_position in zip(config.trolley_ids, insert_positions):
#         width_col = trolley_id + "_estimated_width"
#         width_std_col = trolley_id + "_estimated_width_std"

#         # print(f"width_col    :{width_col}")
#         # print(f"width_std_col:{width_std_col}")

#         width_std_col_values = (
#             dfs[width_col]
#             .rolling(window=window, min_periods=min_periods)
#             .std()
#         )
#         dfs.insert(insert_position + 1, width_std_col, width_std_col_values)

#     return dfs


def load_shelves(rail_fpath, camera_num, base_images, idx):
    image_path = base_images[idx]
    with shelve.open(rail_fpath, writeback=True, encoding="utf-8") as rail:
        trolley_dict = copy.deepcopy(rail[camera_num][image_path])
    return trolley_dict



def default(o):
    # è»Šãƒ¢ãƒ‹ã®æƒ…å ±ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã¨ãã«ä½¿ç”¨
    # JSONã‚’dumpã™ã‚‹ã¨ãã®å¯¾ç­–
    # å‚è€ƒï¼šhttps://qiita.com/yuji38kwmt/items/0a1503f127fc3be17be0
    # print(f"{type(o)=}")
    if isinstance(o, np.int64):
        return int(o)
    elif isinstance(o, np.bool_):
        return bool(o)
    elif isinstance(o, np.ndarray):
        return list(o)
    raise TypeError(repr(o) + " is not JSON serializable")

    
def detect_init_edge(img, x_init):
    # ç”»åƒå…¨ä½“ã®å¹³å‡è¼åº¦ï¼ˆèƒŒæ™¯è¼åº¦ã¨åŒç­‰ã¨ã¿ãªã™ï¼‰ã‚’ç®—å‡º
    img_array = np.array(img)
    img_flat = img_array[:, :, 0].flatten()
    img_random = []
    for i in range(1000): #å…¨ç”»ç´ ã®å¹³å‡ã¯å‡¦ç†æ™‚é–“ã‹ã‹ã‚‹ã®ã§ãƒ©ãƒ³ãƒ€ãƒ 1000ç”»ç´ ã®è¼åº¦å¹³å‡
        x = random.randint(0, 2047999)
        img_random.append(img_flat[x])

    # ãã®ã¾ã¾ã®å¹³å‡è¼åº¦
    avg_brightness1 = round(np.mean(img_random))

    # æ¨™æº–åå·®Ïƒ2ã‚’Maxã«å¤‰æ›
    peakval = avg_brightness1 + np.std(img_flat) * 2.0
    img_random2 = [val if val <= peakval else peakval for val in img_random]

    # æ¨™æº–åå·®Ïƒ2ã‚’Maxã«å¹³å‡è¼åº¦ã‚’èª¿æ•´ï¼ˆãã®ã¾ã¾ã ã¨å°‘ã—é«˜ã‚ã«ãªã£ã¦ã—ã¾ã†ãŸã‚ï¼‰
    avg_brightness2 = round(np.mean(img_random2))
    
    # img_slice = np.copy(img_array[:, 0, 0])
    img_slice = np.copy(img_array[:, x_init, 0])       # x_initã«å¯¾å¿œ

    # åŸºæº–å€¤ã®è¨­å®š
    base_max = max(img_slice).astype(int)              # è¼åº¦Maxå€¤ã€€ï¼ã€€ç”»åƒå…¨ä½“ã®è¼åº¦Maxå€¤
    base_min = int(avg_brightness2)                    # å¹³å‡è¼åº¦

    # è¼åº¦ãƒ‡ãƒ¼ã‚¿ã®å¹³æ»‘åŒ–
    img_smooth1 = np.copy(img_slice)
    img_smooth1[1:2046] = [round(np.mean(img_smooth1[idx-1:idx+2])) for idx in range(1, 2046)]
    
    img_flat = np.std(img_flat)
    ex_center = round((base_max + base_min)/2)
    ex_max = base_max - img_flat
    ex_min = base_min + img_flat

    img_smooth2 = np.copy(img_smooth1)

#     # ä¸€å®šä»¥ä¸Šã®è¼åº¦å¤‰åŒ–ã¯è¼åº¦Maxã‚‚ã—ãã¯å¹³å‡è¼åº¦ã«å¼µã‚Šä»˜ã‘ã‚‹
#     # è¼åº¦å¤‰åŒ–ã®ä½ç½®ã¨å¹…ã‚’è¨˜éŒ²
#     candidate_init = []
#     high_point = [None, None, None, 1]    # ãƒˆãƒ­ãƒªç·šæ‘ºé¢ã‚’è¨˜éŒ²ã™ã‚‹ç®±ï¼ˆé€šå¸¸ã®æ‘ºé¢ãŒå‘¨å›²ã‚ˆã‚Šæ˜ã‚‹ã„å ´åˆï¼‰
#     low_point = [None, None, None, -1]    # ãƒˆãƒ­ãƒªç·šæ‘ºé¢ã‚’è¨˜éŒ²ã™ã‚‹ç®±ï¼ˆè¼åº¦åè»¢ã§æ‘ºé¢ãŒå‘¨å›²ã‚ˆã‚Šæš—ã„å ´åˆï¼‰
#     # for idx in range(len(img_smooth1)):
#     for idx in range(1999):
#         # ä¸­é€”åŠç«¯ãªè¼åº¦ï¼ˆex_centerã‚ˆã‚Šé«˜è¼åº¦ï¼‰ã¯base_maxã«å¤‰æ›
#         if ex_max > img_smooth1[idx] > ex_center:
#             img_smooth2[idx] = base_max
#         # ä¸­é€”åŠç«¯ãªè¼åº¦ï¼ˆex_centerã‚ˆã‚Šä½è¼åº¦ï¼‰ã¯base_minã«å¤‰æ›
#         elif ex_min < img_smooth1[idx] <= ex_center:
#             img_smooth2[idx] = base_min

#         # è¼åº¦ç«‹ã¡ä¸ŠãŒã‚Šç®‡æ‰€ã‚’æ¤œå‡º
#         if idx >= 1 and img_smooth2[idx] >= ex_max and img_smooth2[idx-1] <= ex_min:
#             high_point[0] = idx
#             low_point[1] = idx
#             if low_point[0] != None and abs(low_point[0] - low_point[1]) < 20:                # æ‘ºé¢å¹…ã¨ã—ã¦æ˜ã‚‰ã‹ã«ã‚ã‚Šå¾—ãªã„å ´åˆã¯é™¤ã
#                 low_point[2] = abs(low_point[0] - low_point[1])                               # æ‘ºé¢å¹…
#                 candidate_init.append(low_point)
#             low_point = [None, None, None, -1]
            
#         # è¼åº¦ç«‹ã¡ä¸‹ãŒã‚Šç®‡æ‰€ã‚’æ¤œå‡º
#         elif idx >= 1 and img_smooth2[idx] <= ex_min and img_smooth2[idx-1] >= ex_max:
#             high_point[1] = idx
#             low_point[0] = idx
#             if high_point[0] != None and abs(high_point[0] - high_point[1]) < 20:              # æ‘ºé¢å¹…ã¨ã—ã¦æ˜ã‚‰ã‹ã«ã‚ã‚Šå¾—ãªã„å ´åˆã¯é™¤ã
#                 high_point[2] = abs(high_point[0] - high_point[1])                             # æ‘ºé¢å¹…
#                 candidate_init.append(high_point)
#             high_point = [None, None, None, 1]

    candidate_init = search_candidate(img_smooth1, base_max, base_min, ex_max, ex_min, ex_center)
    
    # ï¼ˆæ¤œè¨ä¸­ï¼‰åˆæœŸå€¤å€™è£œãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã®å¯¾å‡¦
    #           ex_centerã‚’å¤‰ãˆã‚‹ã“ã¨ã§å¯¾å‡¦å¯èƒ½ï¼Ÿï¼Ÿ
    # search_state = False
    # while not search_state:
    #     candidate_init = search_candidate(img_smooth1, base_max, base_min, ex_max, ex_min, ex_center)
    #     candidate_len = len(candidate_init)
    #     if candidate_len != 0:
    #         search_state = True
    #     elif candidate_len == 0:
            
            
    # ï¼ˆè£œæ­£ï¼‰æ¤œå‡ºã—ãŸç‚¹ã‚’å‚¾ãã®ä¸­å¿ƒã«ã™ã‚‹
    # for i, edge in enumerate(search_list):
    for i, edge in enumerate(candidate_init):
        if edge[3] == 1:
            center_u = (max(img_slice[edge[0]-2:edge[0]+8]).astype(np.int16) + min(img_slice[edge[0]-7:edge[0]+3]).astype(np.int16)) / 2
            idx_uu = np.argmin(img_slice[edge[0]-7:edge[0]+3]) + (edge[0]-7)
            idx_ul = np.argmax(img_slice[edge[0]-2:edge[0]+8]) + (edge[0]-2)
            center_l = (max(img_slice[edge[1]-7:edge[1]+3]).astype(np.int16) + min(img_slice[edge[1]-2:edge[1]+8]).astype(np.int16)) / 2
            idx_lu = np.argmax(img_slice[edge[1]-7:edge[1]+3]) + (edge[1]-7)
            idx_ll = np.argmin(img_slice[edge[1]-2:edge[1]+8]) + (edge[1]-2)
        elif edge[3] == -1:
            center_u = (max(img_slice[edge[0]-7:edge[0]+3]).astype(np.int16) + min(img_slice[edge[0]-2:edge[0]+8]).astype(np.int16)) / 2
            idx_uu = np.argmax(img_slice[edge[0]-7:edge[0]+3]) + (edge[0]-7)
            idx_ul = np.argmin(img_slice[edge[0]-2:edge[0]+8]) + (edge[0]-2)
            center_l = (max(img_slice[edge[1]-2:edge[1]+8]).astype(np.int16) + min(img_slice[edge[1]-7:edge[1]+3]).astype(np.int16)) / 2
            idx_lu = np.argmin(img_slice[edge[1]-7:edge[1]+2]) + (edge[1]-7)
            idx_ll = np.argmax(img_slice[edge[1]-2:edge[1]+8]) + (edge[1]-2)
        if idx_uu < idx_ul:
            diff1 = abs(img_slice[idx_uu:idx_ul+1] - center_u)
            idx_u_new = np.argmin(diff1) + idx_uu
        else:
            idx_u_new = edge[0]
        if idx_lu < idx_ll:
            diff2 = abs(img_slice[idx_lu:idx_ll+1] - center_l) 
            idx_l_new = np.argmin(diff2) + idx_lu
        else:
            idx_l_new = edge[1]
        candidate_init[i][0:2] = [idx_u_new, idx_l_new]
    return candidate_init


def search_candidate(img_smooth2, base_max, base_min, ex_max, ex_min, ex_center):
    # ä¸€å®šä»¥ä¸Šã®è¼åº¦å¤‰åŒ–ã¯è¼åº¦Maxã‚‚ã—ãã¯å¹³å‡è¼åº¦ã«å¼µã‚Šä»˜ã‘ã‚‹
    # è¼åº¦å¤‰åŒ–ã®ä½ç½®ã¨å¹…ã‚’è¨˜éŒ²
    candidate_init = []
    high_point = [None, None, None, 1]    # è¼åº¦å¤‰åŒ–ã®ä½ç½®ã‚’è¨˜éŒ²ã™ã‚‹ç®±ï¼ˆå‘¨å›²ã‚ˆã‚Šæ˜ã‚‹ã„å¤‰åŒ–ã®å ´åˆï¼‰
    low_point = [None, None, None, -1]    # è¼åº¦å¤‰åŒ–ã®ä½ç½®ã‚’è¨˜éŒ²ã™ã‚‹ç®±ï¼ˆå‘¨å›²ã‚ˆã‚Šæš—ã„å¤‰åŒ–ã®å ´åˆï¼‰ â€»è¼åº¦åè»¢
    # for idx in range(len(img_smooth1)):
    for idx in range(1999):
        # ä¸­é€”åŠç«¯ãªè¼åº¦ï¼ˆex_centerã‚ˆã‚Šé«˜è¼åº¦ï¼‰ã¯base_maxã«å¤‰æ›
        if ex_max > img_smooth2[idx] > ex_center:
            img_smooth2[idx] = base_max
        # ä¸­é€”åŠç«¯ãªè¼åº¦ï¼ˆex_centerã‚ˆã‚Šä½è¼åº¦ï¼‰ã¯base_minã«å¤‰æ›
        elif ex_min < img_smooth2[idx] <= ex_center:
            img_smooth2[idx] = base_min

        # è¼åº¦ç«‹ã¡ä¸ŠãŒã‚Šç®‡æ‰€ã‚’æ¤œå‡º
        if idx >= 1 and img_smooth2[idx] >= ex_max and img_smooth2[idx-1] <= ex_min:
            high_point[0] = idx
            low_point[1] = idx
            if low_point[0] != None and abs(low_point[0] - low_point[1]) < 20:                # æ‘ºé¢å¹…ã¨ã—ã¦æ˜ã‚‰ã‹ã«ã‚ã‚Šå¾—ãªã„å ´åˆã¯é™¤ã
                low_point[2] = abs(low_point[0] - low_point[1])                               # æ‘ºé¢å¹…
                candidate_init.append(low_point)
            low_point = [None, None, None, -1]
            
        # è¼åº¦ç«‹ã¡ä¸‹ãŒã‚Šç®‡æ‰€ã‚’æ¤œå‡º
        elif idx >= 1 and img_smooth2[idx] <= ex_min and img_smooth2[idx-1] >= ex_max:
            high_point[1] = idx
            low_point[0] = idx
            if high_point[0] != None and abs(high_point[0] - high_point[1]) < 20:              # æ‘ºé¢å¹…ã¨ã—ã¦æ˜ã‚‰ã‹ã«ã‚ã‚Šå¾—ãªã„å ´åˆã¯é™¤ã
                high_point[2] = abs(high_point[0] - high_point[1])                             # æ‘ºé¢å¹…
                candidate_init.append(high_point)
            high_point = [None, None, None, 1]
    return candidate_init


def search_csv(outpath):                 # 2024.5.22 -->
    exist_csv = False
    list_csv = list_csvs(outpath)
    for file in list_csv:
        if 'rail_' and '.csv' in file:
            exist_csv = True
            break
    return exist_csv                     # --> 2024.5.22